{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSPSR CHIKUSI NOVAL\n",
    "import pdb\n",
    "import math\n",
    "import time  \n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "# from scipy.misc import imresize\n",
    "from scipy.signal import convolve2d\n",
    "from skimage.measure import compare_psnr, compare_ssim\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchnet import meter\n",
    "\n",
    "\n",
    "# from data_utils import TrainsetFromFolder, ValsetFromFolder\n",
    "# from data_utils import is_image_file\n",
    "# from data_utils2 import HSTrainingData, HSTestData\n",
    "# from data_utils3 import loadingData,loadingTestData\n",
    "from data_utils4 import TrainsetFromFolder, ValsetFromFolder\n",
    "from data_utils4 import is_image_file\n",
    "\n",
    "from eval import PSNR, SSIM, SAM\n",
    "# from eval2 import quality_assessment\n",
    "\n",
    "from loss import HybridLoss, SAMLoss\n",
    "# from model.MCNet import MCNet\n",
    "# from model.SSAC import SSAC\n",
    "# from model.SSPSR import SSPSR\n",
    "# from model.SSPSR import default_conv\n",
    "from LGFFMN import LGFFMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "# without val\n",
    "parser = argparse.ArgumentParser(description=\"Super-Resolution\")\n",
    "\n",
    "parser.add_argument(\"--model_title\", type=str, default=\"WTSR00\", help=\"model_title, default set to model_title\")\n",
    "# parser.add_argument('--method', default='ASFS', type=str, help='super resolution method name')\n",
    "\n",
    "# Training settings\n",
    "# parser.add_argument(\"--upscale_factor\", default=4, type=int, help=\"super resolution upscale factor\")\n",
    "parser.add_argument(\"--upscale_factor\", default=4, type=int, help=\"super resolution upscale factor\")\n",
    "parser.add_argument(\"--seed\", type=int, default=3000, help=\"start seed for model\")\n",
    "parser.add_argument(\"--batchSize\", type=int, default=32, help=\"training batch size\")\n",
    "# parser.add_argument(\"--batchSize\", type=int, default=64, help=\"training batch size\")\n",
    "# parser.add_argument(\"--nEpochs\", type=int, default=85, help=\"maximum number of epochs to train\")\n",
    "parser.add_argument(\"--nEpochs\", type=int, default=200, help=\"maximum number of epochs to train\")\n",
    "# parser.add_argument(\"--show\", action=\"store_true\", help=\"show Tensorboard\")\n",
    "parser.add_argument(\"--show\", type=int, default=1, help=\"show Tensorboard\")\n",
    "parser.add_argument(\"--lr\", type=int, default=1e-4, help=\"lerning rate\")\n",
    "# parser.add_argument(\"--cuda\", action=\"store_true\", help=\"Use cuda\")\n",
    "parser.add_argument(\"--cuda\", type=int, required=False,default=1, help=\"set it to 1 for running on GPU, 0 for CPU\")\n",
    "# parser.add_argument(\"--gpus\", default=\"0,1,2,3\", type=str, help=\"gpu ids (default: 0)\")\n",
    "parser.add_argument(\"--gpus\", default=\"2,3\", type=str, help=\"gpu ids (default: 0)\")\n",
    "parser.add_argument(\"--threads\", type=int, default=12, help=\"number of threads for dataloader to use\")\n",
    "parser.add_argument(\"--resume\", default=\"\", type=str, help=\"Path to checkpoint (default: none)  checkpoint/model_epoch_95.pth\")\n",
    "parser.add_argument(\"--start-epoch\", default=1, type=int, help=\"Manual epoch number (useful on restarts)\")               \n",
    "parser.add_argument(\"--datasetName\", default=\"CAVE\", type=str, help=\"data name\")\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=0, help=\"weight decay, default set to 0\")\n",
    "\n",
    "\n",
    "\n",
    "#SFCSR # Network settings\n",
    "\n",
    "# parser.add_argument('--n_module', type=int, default=5, help='number of  modules')\n",
    "# parser.add_argument('--n_feats', type=int, default=64, help='number of feature maps')\n",
    "\n",
    "\n",
    "\n",
    "# Test image\n",
    "# parser.add_argument('--model_name', default='checkpoint/CAVE_model_4_epoch_XX.pth', type=str, help='super resolution model name ')\n",
    "parser.add_argument('--model_name', default='', type=str, help='super resolution model name ')\n",
    "\n",
    "\n",
    "# opt = parser.parse_args() \n",
    "opt = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global writer\n",
    "write_path ='/media/dy113/disk1/Project_xjr/hyperfusion/Mine/logs/'+ opt.model_title + '/' + opt.datasetName + '/' + str(opt.upscale_factor) + '/' + 'nfeats128/'\n",
    "writer = SummaryWriter(write_path)\n",
    "# writer = SummaryWriter('./logs/') \n",
    "# out_path = '/home/sunjianjun/liqiang/Dataset/test/' + opt.datasetName + '/' \n",
    "# out_path = '/media/dy113/disk1/Project_xjr/hyperfusion/Mine/result/' + opt.model_title + '/' + opt.datasetName + '/' + opt.upscale_factor + '/'\n",
    "resume = True\n",
    "# resume = False\n",
    "log_interval = 50\n",
    "psnr = []   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Use GPU ID: '2,3'\n",
      "# parameters: 7862856\n",
      "=> loading checkpoint '/media/dy113/disk1/Project_xjr/hyperfusion/Mine/checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_85.pth'\n",
      "Epoch = 86, lr = 5e-05\n",
      "===> Epoch[86](50/248): Loss: 0.0106652901 Loss1: 0.0061360635 Loss2: 0.0045292270\n",
      "===> Epoch[86](100/248): Loss: 0.0134341503 Loss1: 0.0090000145 Loss2: 0.0044341357\n",
      "===> Epoch[86](150/248): Loss: 0.0119777005 Loss1: 0.0074883611 Loss2: 0.0044893394\n",
      "===> Epoch[86](200/248): Loss: 0.0114008691 Loss1: 0.0071346615 Loss2: 0.0042662071\n",
      "PSNR = 37.187\n",
      "Epoch = 87, lr = 5e-05\n",
      "===> Epoch[87](50/248): Loss: 0.0117827114 Loss1: 0.0073879845 Loss2: 0.0043947273\n",
      "===> Epoch[87](100/248): Loss: 0.0149134099 Loss1: 0.0097671328 Loss2: 0.0051462776\n",
      "===> Epoch[87](150/248): Loss: 0.0120050870 Loss1: 0.0073162615 Loss2: 0.0046888250\n",
      "===> Epoch[87](200/248): Loss: 0.0133825857 Loss1: 0.0084159086 Loss2: 0.0049666776\n",
      "PSNR = 37.210\n",
      "Epoch = 88, lr = 5e-05\n",
      "===> Epoch[88](50/248): Loss: 0.0139402328 Loss1: 0.0089638671 Loss2: 0.0049763657\n",
      "===> Epoch[88](100/248): Loss: 0.0093816444 Loss1: 0.0050529549 Loss2: 0.0043286900\n",
      "===> Epoch[88](150/248): Loss: 0.0117127690 Loss1: 0.0071427338 Loss2: 0.0045700353\n",
      "===> Epoch[88](200/248): Loss: 0.0123620369 Loss1: 0.0079361191 Loss2: 0.0044259182\n",
      "PSNR = 37.169\n",
      "Epoch = 89, lr = 5e-05\n",
      "===> Epoch[89](50/248): Loss: 0.0121304905 Loss1: 0.0077450206 Loss2: 0.0043854699\n",
      "===> Epoch[89](100/248): Loss: 0.0140800532 Loss1: 0.0093370024 Loss2: 0.0047430503\n",
      "===> Epoch[89](150/248): Loss: 0.0138790421 Loss1: 0.0084754806 Loss2: 0.0054035620\n",
      "===> Epoch[89](200/248): Loss: 0.0104735438 Loss1: 0.0057712784 Loss2: 0.0047022649\n",
      "PSNR = 37.157\n",
      "Epoch = 90, lr = 5e-05\n",
      "===> Epoch[90](50/248): Loss: 0.0112638436 Loss1: 0.0067388183 Loss2: 0.0045250258\n",
      "===> Epoch[90](100/248): Loss: 0.0117975101 Loss1: 0.0072173262 Loss2: 0.0045801844\n",
      "===> Epoch[90](150/248): Loss: 0.0126479231 Loss1: 0.0078036822 Loss2: 0.0048442404\n",
      "===> Epoch[90](200/248): Loss: 0.0147584565 Loss1: 0.0092919962 Loss2: 0.0054664598\n",
      "PSNR = 37.211\n",
      "Checkpoint saved to ./checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_90.pth\n",
      "Epoch = 91, lr = 5e-05\n",
      "===> Epoch[91](50/248): Loss: 0.0135441888 Loss1: 0.0088296030 Loss2: 0.0047145863\n",
      "===> Epoch[91](100/248): Loss: 0.0141445864 Loss1: 0.0089715905 Loss2: 0.0051729963\n",
      "===> Epoch[91](150/248): Loss: 0.0109246150 Loss1: 0.0067752753 Loss2: 0.0041493401\n",
      "===> Epoch[91](200/248): Loss: 0.0105263386 Loss1: 0.0061067124 Loss2: 0.0044196257\n",
      "PSNR = 37.180\n",
      "Epoch = 92, lr = 5e-05\n",
      "===> Epoch[92](50/248): Loss: 0.0112481834 Loss1: 0.0066325180 Loss2: 0.0046156654\n",
      "===> Epoch[92](100/248): Loss: 0.0118979458 Loss1: 0.0069534201 Loss2: 0.0049445261\n",
      "===> Epoch[92](150/248): Loss: 0.0115671987 Loss1: 0.0071537495 Loss2: 0.0044134492\n",
      "===> Epoch[92](200/248): Loss: 0.0151730999 Loss1: 0.0098906411 Loss2: 0.0052824593\n",
      "PSNR = 37.221\n",
      "Epoch = 93, lr = 5e-05\n",
      "===> Epoch[93](50/248): Loss: 0.0113147981 Loss1: 0.0070656845 Loss2: 0.0042491141\n",
      "===> Epoch[93](100/248): Loss: 0.0118789990 Loss1: 0.0077715334 Loss2: 0.0041074650\n",
      "===> Epoch[93](150/248): Loss: 0.0104203206 Loss1: 0.0062311953 Loss2: 0.0041891257\n",
      "===> Epoch[93](200/248): Loss: 0.0142025575 Loss1: 0.0094101178 Loss2: 0.0047924402\n",
      "PSNR = 37.192\n",
      "Epoch = 94, lr = 5e-05\n",
      "===> Epoch[94](50/248): Loss: 0.0111413011 Loss1: 0.0068535670 Loss2: 0.0042877342\n",
      "===> Epoch[94](100/248): Loss: 0.0111673549 Loss1: 0.0065908358 Loss2: 0.0045765187\n",
      "===> Epoch[94](150/248): Loss: 0.0134597179 Loss1: 0.0084196050 Loss2: 0.0050401124\n",
      "===> Epoch[94](200/248): Loss: 0.0135614797 Loss1: 0.0084867897 Loss2: 0.0050746906\n",
      "PSNR = 37.144\n",
      "Epoch = 95, lr = 5e-05\n",
      "===> Epoch[95](50/248): Loss: 0.0106080361 Loss1: 0.0063935136 Loss2: 0.0042145224\n",
      "===> Epoch[95](100/248): Loss: 0.0117957741 Loss1: 0.0078053400 Loss2: 0.0039904346\n",
      "===> Epoch[95](150/248): Loss: 0.0119574256 Loss1: 0.0078205755 Loss2: 0.0041368501\n",
      "===> Epoch[95](200/248): Loss: 0.0108404048 Loss1: 0.0068713580 Loss2: 0.0039690468\n",
      "PSNR = 37.207\n",
      "Checkpoint saved to ./checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_95.pth\n",
      "Epoch = 96, lr = 5e-05\n",
      "===> Epoch[96](50/248): Loss: 0.0106516331 Loss1: 0.0065458673 Loss2: 0.0041057663\n",
      "===> Epoch[96](100/248): Loss: 0.0118929185 Loss1: 0.0074734185 Loss2: 0.0044194995\n",
      "===> Epoch[96](150/248): Loss: 0.0111620594 Loss1: 0.0062729246 Loss2: 0.0048891343\n",
      "===> Epoch[96](200/248): Loss: 0.0109535083 Loss1: 0.0065044439 Loss2: 0.0044490644\n",
      "PSNR = 37.262\n",
      "Epoch = 97, lr = 5e-05\n",
      "===> Epoch[97](50/248): Loss: 0.0111072427 Loss1: 0.0068917009 Loss2: 0.0042155418\n",
      "===> Epoch[97](100/248): Loss: 0.0139311086 Loss1: 0.0091443844 Loss2: 0.0047867242\n",
      "===> Epoch[97](150/248): Loss: 0.0117581477 Loss1: 0.0072570061 Loss2: 0.0045011416\n",
      "===> Epoch[97](200/248): Loss: 0.0108466353 Loss1: 0.0060431259 Loss2: 0.0048035095\n",
      "PSNR = 37.178\n",
      "Epoch = 98, lr = 5e-05\n",
      "===> Epoch[98](50/248): Loss: 0.0103988191 Loss1: 0.0052647395 Loss2: 0.0051340796\n",
      "===> Epoch[98](100/248): Loss: 0.0138917668 Loss1: 0.0092929481 Loss2: 0.0045988187\n",
      "===> Epoch[98](150/248): Loss: 0.0113140782 Loss1: 0.0066032629 Loss2: 0.0047108154\n",
      "===> Epoch[98](200/248): Loss: 0.0134534771 Loss1: 0.0089811077 Loss2: 0.0044723693\n",
      "PSNR = 37.159\n",
      "Epoch = 99, lr = 5e-05\n",
      "===> Epoch[99](50/248): Loss: 0.0121329548 Loss1: 0.0076397192 Loss2: 0.0044932356\n",
      "===> Epoch[99](100/248): Loss: 0.0116334781 Loss1: 0.0071812617 Loss2: 0.0044522160\n",
      "===> Epoch[99](150/248): Loss: 0.0098992493 Loss1: 0.0055199298 Loss2: 0.0043793195\n",
      "===> Epoch[99](200/248): Loss: 0.0112741757 Loss1: 0.0068649049 Loss2: 0.0044092704\n",
      "PSNR = 37.203\n",
      "Epoch = 100, lr = 5e-05\n",
      "===> Epoch[100](50/248): Loss: 0.0100154579 Loss1: 0.0058974889 Loss2: 0.0041179690\n",
      "===> Epoch[100](100/248): Loss: 0.0120249791 Loss1: 0.0077431439 Loss2: 0.0042818352\n",
      "===> Epoch[100](150/248): Loss: 0.0110384291 Loss1: 0.0065241214 Loss2: 0.0045143077\n",
      "===> Epoch[100](200/248): Loss: 0.0121908914 Loss1: 0.0073926179 Loss2: 0.0047982740\n",
      "PSNR = 37.200\n",
      "Checkpoint saved to ./checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_100.pth\n",
      "Epoch = 101, lr = 5e-05\n",
      "===> Epoch[101](50/248): Loss: 0.0134274382 Loss1: 0.0086300885 Loss2: 0.0047973492\n",
      "===> Epoch[101](100/248): Loss: 0.0109892357 Loss1: 0.0069139390 Loss2: 0.0040752972\n",
      "===> Epoch[101](150/248): Loss: 0.0116113201 Loss1: 0.0069819517 Loss2: 0.0046293689\n",
      "===> Epoch[101](200/248): Loss: 0.0105087887 Loss1: 0.0059516779 Loss2: 0.0045571104\n",
      "PSNR = 37.234\n",
      "Epoch = 102, lr = 5e-05\n",
      "===> Epoch[102](50/248): Loss: 0.0112504233 Loss1: 0.0069920640 Loss2: 0.0042583589\n",
      "===> Epoch[102](100/248): Loss: 0.0114087788 Loss1: 0.0069301156 Loss2: 0.0044786632\n",
      "===> Epoch[102](150/248): Loss: 0.0129769500 Loss1: 0.0082138497 Loss2: 0.0047631003\n",
      "===> Epoch[102](200/248): Loss: 0.0123301214 Loss1: 0.0075215381 Loss2: 0.0048085833\n",
      "PSNR = 37.167\n",
      "Epoch = 103, lr = 5e-05\n",
      "===> Epoch[103](50/248): Loss: 0.0125792269 Loss1: 0.0079817260 Loss2: 0.0045975009\n",
      "===> Epoch[103](100/248): Loss: 0.0121487072 Loss1: 0.0078502800 Loss2: 0.0042984271\n",
      "===> Epoch[103](150/248): Loss: 0.0120650586 Loss1: 0.0075704483 Loss2: 0.0044946107\n",
      "===> Epoch[103](200/248): Loss: 0.0113409851 Loss1: 0.0069128340 Loss2: 0.0044281511\n",
      "PSNR = 37.184\n",
      "Epoch = 104, lr = 5e-05\n",
      "===> Epoch[104](50/248): Loss: 0.0110118948 Loss1: 0.0066487384 Loss2: 0.0043631564\n",
      "===> Epoch[104](100/248): Loss: 0.0129373325 Loss1: 0.0083053187 Loss2: 0.0046320138\n",
      "===> Epoch[104](150/248): Loss: 0.0147441495 Loss1: 0.0094374530 Loss2: 0.0053066961\n",
      "===> Epoch[104](200/248): Loss: 0.0122284899 Loss1: 0.0076163732 Loss2: 0.0046121166\n",
      "PSNR = 37.141\n",
      "Epoch = 105, lr = 5e-05\n",
      "===> Epoch[105](50/248): Loss: 0.0104734618 Loss1: 0.0059376298 Loss2: 0.0045358315\n",
      "===> Epoch[105](100/248): Loss: 0.0143497325 Loss1: 0.0088056205 Loss2: 0.0055441125\n",
      "===> Epoch[105](150/248): Loss: 0.0094808945 Loss1: 0.0051659504 Loss2: 0.0043149441\n",
      "===> Epoch[105](200/248): Loss: 0.0106700491 Loss1: 0.0065316614 Loss2: 0.0041383873\n",
      "PSNR = 37.189\n",
      "Checkpoint saved to ./checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_105.pth\n",
      "Epoch = 106, lr = 5e-05\n",
      "===> Epoch[106](50/248): Loss: 0.0134326071 Loss1: 0.0087868562 Loss2: 0.0046457509\n",
      "===> Epoch[106](100/248): Loss: 0.0113256834 Loss1: 0.0071588657 Loss2: 0.0041668178\n",
      "===> Epoch[106](150/248): Loss: 0.0150115676 Loss1: 0.0095047820 Loss2: 0.0055067851\n",
      "===> Epoch[106](200/248): Loss: 0.0112628955 Loss1: 0.0070848386 Loss2: 0.0041780570\n",
      "PSNR = 37.215\n",
      "Epoch = 107, lr = 5e-05\n",
      "===> Epoch[107](50/248): Loss: 0.0120642548 Loss1: 0.0076923762 Loss2: 0.0043718787\n",
      "===> Epoch[107](100/248): Loss: 0.0137229059 Loss1: 0.0091966782 Loss2: 0.0045262282\n",
      "===> Epoch[107](150/248): Loss: 0.0108739045 Loss1: 0.0071750060 Loss2: 0.0036988989\n",
      "===> Epoch[107](200/248): Loss: 0.0119292187 Loss1: 0.0074996040 Loss2: 0.0044296146\n",
      "PSNR = 37.196\n",
      "Epoch = 108, lr = 5e-05\n",
      "===> Epoch[108](50/248): Loss: 0.0101457741 Loss1: 0.0056141182 Loss2: 0.0045316564\n",
      "===> Epoch[108](100/248): Loss: 0.0123918299 Loss1: 0.0080837002 Loss2: 0.0043081292\n",
      "===> Epoch[108](150/248): Loss: 0.0096046980 Loss1: 0.0052347556 Loss2: 0.0043699425\n",
      "===> Epoch[108](200/248): Loss: 0.0139524052 Loss1: 0.0086167753 Loss2: 0.0053356299\n",
      "PSNR = 37.215\n",
      "Epoch = 109, lr = 5e-05\n",
      "===> Epoch[109](50/248): Loss: 0.0083418339 Loss1: 0.0045071142 Loss2: 0.0038347200\n",
      "===> Epoch[109](100/248): Loss: 0.0134002939 Loss1: 0.0083950413 Loss2: 0.0050052525\n",
      "===> Epoch[109](150/248): Loss: 0.0123296101 Loss1: 0.0076839468 Loss2: 0.0046456628\n",
      "===> Epoch[109](200/248): Loss: 0.0110069867 Loss1: 0.0064975591 Loss2: 0.0045094276\n",
      "PSNR = 37.195\n",
      "Epoch = 110, lr = 5e-05\n",
      "===> Epoch[110](50/248): Loss: 0.0098934388 Loss1: 0.0061390512 Loss2: 0.0037543874\n",
      "===> Epoch[110](100/248): Loss: 0.0120764729 Loss1: 0.0076343836 Loss2: 0.0044420897\n",
      "===> Epoch[110](150/248): Loss: 0.0137509927 Loss1: 0.0089819562 Loss2: 0.0047690361\n",
      "===> Epoch[110](200/248): Loss: 0.0101748686 Loss1: 0.0061787912 Loss2: 0.0039960775\n",
      "PSNR = 37.232\n",
      "Checkpoint saved to ./checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_110.pth\n",
      "Epoch = 111, lr = 5e-05\n",
      "===> Epoch[111](50/248): Loss: 0.0124272201 Loss1: 0.0079097934 Loss2: 0.0045174262\n",
      "===> Epoch[111](100/248): Loss: 0.0124795400 Loss1: 0.0078109493 Loss2: 0.0046685911\n",
      "===> Epoch[111](150/248): Loss: 0.0131069627 Loss1: 0.0085926922 Loss2: 0.0045142705\n",
      "===> Epoch[111](200/248): Loss: 0.0129537098 Loss1: 0.0082628382 Loss2: 0.0046908716\n",
      "PSNR = 37.173\n",
      "Epoch = 112, lr = 5e-05\n",
      "===> Epoch[112](50/248): Loss: 0.0111662522 Loss1: 0.0070469687 Loss2: 0.0041192840\n",
      "===> Epoch[112](100/248): Loss: 0.0102890469 Loss1: 0.0062497947 Loss2: 0.0040392517\n",
      "===> Epoch[112](150/248): Loss: 0.0114516113 Loss1: 0.0069715055 Loss2: 0.0044801058\n",
      "===> Epoch[112](200/248): Loss: 0.0140571669 Loss1: 0.0083146645 Loss2: 0.0057425029\n",
      "PSNR = 37.244\n",
      "Epoch = 113, lr = 5e-05\n",
      "===> Epoch[113](50/248): Loss: 0.0109661277 Loss1: 0.0064238682 Loss2: 0.0045422590\n",
      "===> Epoch[113](100/248): Loss: 0.0111828391 Loss1: 0.0070052575 Loss2: 0.0041775820\n",
      "===> Epoch[113](150/248): Loss: 0.0110622225 Loss1: 0.0062174676 Loss2: 0.0048447549\n",
      "===> Epoch[113](200/248): Loss: 0.0134710148 Loss1: 0.0087889638 Loss2: 0.0046820506\n",
      "PSNR = 37.154\n",
      "Epoch = 114, lr = 5e-05\n",
      "===> Epoch[114](50/248): Loss: 0.0093305763 Loss1: 0.0050426833 Loss2: 0.0042878934\n",
      "===> Epoch[114](100/248): Loss: 0.0131963082 Loss1: 0.0084186178 Loss2: 0.0047776909\n",
      "===> Epoch[114](150/248): Loss: 0.0101764705 Loss1: 0.0061487467 Loss2: 0.0040277238\n",
      "===> Epoch[114](200/248): Loss: 0.0118260989 Loss1: 0.0071881698 Loss2: 0.0046379291\n",
      "PSNR = 37.208\n",
      "Epoch = 115, lr = 5e-05\n",
      "===> Epoch[115](50/248): Loss: 0.0091345245 Loss1: 0.0053459788 Loss2: 0.0037885455\n",
      "===> Epoch[115](100/248): Loss: 0.0112674739 Loss1: 0.0069233524 Loss2: 0.0043441216\n",
      "===> Epoch[115](150/248): Loss: 0.0101199374 Loss1: 0.0056297239 Loss2: 0.0044902130\n",
      "===> Epoch[115](200/248): Loss: 0.0141053144 Loss1: 0.0092523023 Loss2: 0.0048530116\n",
      "PSNR = 37.223\n",
      "Checkpoint saved to ./checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_115.pth\n",
      "Epoch = 116, lr = 5e-05\n",
      "===> Epoch[116](50/248): Loss: 0.0120660271 Loss1: 0.0074320734 Loss2: 0.0046339538\n",
      "===> Epoch[116](100/248): Loss: 0.0096770879 Loss1: 0.0061714230 Loss2: 0.0035056651\n",
      "===> Epoch[116](150/248): Loss: 0.0106357075 Loss1: 0.0059393374 Loss2: 0.0046963701\n",
      "===> Epoch[116](200/248): Loss: 0.0117294090 Loss1: 0.0072245891 Loss2: 0.0045048199\n",
      "PSNR = 37.260\n",
      "Epoch = 117, lr = 5e-05\n",
      "===> Epoch[117](50/248): Loss: 0.0103362761 Loss1: 0.0060304184 Loss2: 0.0043058577\n",
      "===> Epoch[117](100/248): Loss: 0.0116230370 Loss1: 0.0071129710 Loss2: 0.0045100660\n",
      "===> Epoch[117](150/248): Loss: 0.0104054855 Loss1: 0.0060408781 Loss2: 0.0043646074\n",
      "===> Epoch[117](200/248): Loss: 0.0130343810 Loss1: 0.0081799319 Loss2: 0.0048544486\n",
      "PSNR = 37.270\n",
      "Epoch = 118, lr = 5e-05\n",
      "===> Epoch[118](50/248): Loss: 0.0128894392 Loss1: 0.0079278704 Loss2: 0.0049615689\n",
      "===> Epoch[118](100/248): Loss: 0.0121663557 Loss1: 0.0078150518 Loss2: 0.0043513039\n",
      "===> Epoch[118](150/248): Loss: 0.0098367762 Loss1: 0.0059937527 Loss2: 0.0038430230\n",
      "===> Epoch[118](200/248): Loss: 0.0115543399 Loss1: 0.0069702026 Loss2: 0.0045841369\n",
      "PSNR = 37.237\n",
      "Epoch = 119, lr = 5e-05\n",
      "===> Epoch[119](50/248): Loss: 0.0094123287 Loss1: 0.0057255421 Loss2: 0.0036867864\n",
      "===> Epoch[119](100/248): Loss: 0.0108297374 Loss1: 0.0066974489 Loss2: 0.0041322890\n",
      "===> Epoch[119](150/248): Loss: 0.0117989546 Loss1: 0.0076819840 Loss2: 0.0041169706\n",
      "===> Epoch[119](200/248): Loss: 0.0112518892 Loss1: 0.0067336475 Loss2: 0.0045182421\n",
      "PSNR = 37.255\n",
      "Epoch = 120, lr = 5e-05\n",
      "===> Epoch[120](50/248): Loss: 0.0129589606 Loss1: 0.0082553215 Loss2: 0.0047036391\n",
      "===> Epoch[120](100/248): Loss: 0.0115397889 Loss1: 0.0074607856 Loss2: 0.0040790029\n",
      "===> Epoch[120](150/248): Loss: 0.0096927211 Loss1: 0.0053345016 Loss2: 0.0043582194\n",
      "===> Epoch[120](200/248): Loss: 0.0081504863 Loss1: 0.0046379939 Loss2: 0.0035124929\n",
      "PSNR = 37.255\n",
      "Checkpoint saved to ./checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_120.pth\n",
      "Epoch = 121, lr = 2.5e-05\n",
      "===> Epoch[121](50/248): Loss: 0.0123588759 Loss1: 0.0077961287 Loss2: 0.0045627467\n",
      "===> Epoch[121](100/248): Loss: 0.0124502638 Loss1: 0.0077114259 Loss2: 0.0047388379\n",
      "===> Epoch[121](150/248): Loss: 0.0111999223 Loss1: 0.0070664431 Loss2: 0.0041334792\n",
      "===> Epoch[121](200/248): Loss: 0.0085920952 Loss1: 0.0048770239 Loss2: 0.0037150711\n",
      "PSNR = 37.269\n",
      "Epoch = 122, lr = 2.5e-05\n",
      "===> Epoch[122](50/248): Loss: 0.0094334893 Loss1: 0.0053745145 Loss2: 0.0040589753\n",
      "===> Epoch[122](100/248): Loss: 0.0103878202 Loss1: 0.0062017236 Loss2: 0.0041860966\n",
      "===> Epoch[122](150/248): Loss: 0.0142458919 Loss1: 0.0089150248 Loss2: 0.0053308671\n",
      "===> Epoch[122](200/248): Loss: 0.0108914282 Loss1: 0.0064531756 Loss2: 0.0044382527\n",
      "PSNR = 37.241\n",
      "Epoch = 123, lr = 2.5e-05\n",
      "===> Epoch[123](50/248): Loss: 0.0099560600 Loss1: 0.0058882781 Loss2: 0.0040677823\n",
      "===> Epoch[123](100/248): Loss: 0.0127487015 Loss1: 0.0082182009 Loss2: 0.0045305006\n",
      "===> Epoch[123](150/248): Loss: 0.0121270716 Loss1: 0.0075534503 Loss2: 0.0045736209\n",
      "===> Epoch[123](200/248): Loss: 0.0127130989 Loss1: 0.0078075556 Loss2: 0.0049055433\n",
      "PSNR = 37.264\n",
      "Epoch = 124, lr = 2.5e-05\n",
      "===> Epoch[124](50/248): Loss: 0.0098341312 Loss1: 0.0055639683 Loss2: 0.0042701624\n",
      "===> Epoch[124](100/248): Loss: 0.0128732324 Loss1: 0.0078134453 Loss2: 0.0050597875\n",
      "===> Epoch[124](150/248): Loss: 0.0097105354 Loss1: 0.0055141719 Loss2: 0.0041963640\n",
      "===> Epoch[124](200/248): Loss: 0.0106854197 Loss1: 0.0069017583 Loss2: 0.0037836619\n",
      "PSNR = 37.257\n",
      "Epoch = 125, lr = 2.5e-05\n",
      "===> Epoch[125](50/248): Loss: 0.0121184755 Loss1: 0.0079798438 Loss2: 0.0041386317\n",
      "===> Epoch[125](100/248): Loss: 0.0095572732 Loss1: 0.0053722681 Loss2: 0.0041850046\n",
      "===> Epoch[125](150/248): Loss: 0.0102655329 Loss1: 0.0062139034 Loss2: 0.0040516290\n",
      "===> Epoch[125](200/248): Loss: 0.0098699853 Loss1: 0.0056997035 Loss2: 0.0041702813\n",
      "PSNR = 37.275\n",
      "Checkpoint saved to ./checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_125.pth\n",
      "Epoch = 126, lr = 2.5e-05\n",
      "===> Epoch[126](50/248): Loss: 0.0135385254 Loss1: 0.0089364685 Loss2: 0.0046020569\n",
      "===> Epoch[126](100/248): Loss: 0.0111471415 Loss1: 0.0067971577 Loss2: 0.0043499838\n",
      "===> Epoch[126](150/248): Loss: 0.0102244429 Loss1: 0.0057209940 Loss2: 0.0045034494\n",
      "===> Epoch[126](200/248): Loss: 0.0101579931 Loss1: 0.0065458356 Loss2: 0.0036121570\n",
      "PSNR = 37.258\n",
      "Epoch = 127, lr = 2.5e-05\n",
      "===> Epoch[127](50/248): Loss: 0.0124980547 Loss1: 0.0079464447 Loss2: 0.0045516095\n",
      "===> Epoch[127](100/248): Loss: 0.0099520404 Loss1: 0.0053448472 Loss2: 0.0046071927\n",
      "===> Epoch[127](150/248): Loss: 0.0088573154 Loss1: 0.0050320718 Loss2: 0.0038252436\n",
      "===> Epoch[127](200/248): Loss: 0.0103318356 Loss1: 0.0061537623 Loss2: 0.0041780733\n",
      "PSNR = 37.263\n",
      "Epoch = 128, lr = 2.5e-05\n",
      "===> Epoch[128](50/248): Loss: 0.0118096173 Loss1: 0.0072097946 Loss2: 0.0045998232\n",
      "===> Epoch[128](100/248): Loss: 0.0120011847 Loss1: 0.0078258701 Loss2: 0.0041753142\n",
      "===> Epoch[128](150/248): Loss: 0.0109575093 Loss1: 0.0066752345 Loss2: 0.0042822743\n",
      "===> Epoch[128](200/248): Loss: 0.0101820435 Loss1: 0.0063131801 Loss2: 0.0038688637\n",
      "PSNR = 37.270\n",
      "Epoch = 129, lr = 2.5e-05\n",
      "===> Epoch[129](50/248): Loss: 0.0097814854 Loss1: 0.0057109534 Loss2: 0.0040705325\n",
      "===> Epoch[129](100/248): Loss: 0.0115314834 Loss1: 0.0071799750 Loss2: 0.0043515079\n",
      "===> Epoch[129](150/248): Loss: 0.0103054019 Loss1: 0.0059192381 Loss2: 0.0043861638\n",
      "===> Epoch[129](200/248): Loss: 0.0112989461 Loss1: 0.0064488552 Loss2: 0.0048500909\n",
      "PSNR = 37.282\n",
      "Epoch = 130, lr = 2.5e-05\n",
      "===> Epoch[130](50/248): Loss: 0.0112677589 Loss1: 0.0067657442 Loss2: 0.0045020152\n",
      "===> Epoch[130](100/248): Loss: 0.0090752998 Loss1: 0.0054066065 Loss2: 0.0036686931\n",
      "===> Epoch[130](150/248): Loss: 0.0121683637 Loss1: 0.0073994636 Loss2: 0.0047689001\n",
      "===> Epoch[130](200/248): Loss: 0.0089134891 Loss1: 0.0051773177 Loss2: 0.0037361712\n",
      "PSNR = 37.237\n",
      "Checkpoint saved to ./checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_130.pth\n",
      "Epoch = 131, lr = 2.5e-05\n",
      "===> Epoch[131](50/248): Loss: 0.0123043209 Loss1: 0.0076282453 Loss2: 0.0046760761\n",
      "===> Epoch[131](100/248): Loss: 0.0107948817 Loss1: 0.0064125489 Loss2: 0.0043823323\n",
      "===> Epoch[131](150/248): Loss: 0.0092150923 Loss1: 0.0052295071 Loss2: 0.0039855852\n",
      "===> Epoch[131](200/248): Loss: 0.0124164373 Loss1: 0.0075096139 Loss2: 0.0049068239\n",
      "PSNR = 37.286\n",
      "Epoch = 132, lr = 2.5e-05\n",
      "===> Epoch[132](50/248): Loss: 0.0100895390 Loss1: 0.0054732072 Loss2: 0.0046163318\n",
      "===> Epoch[132](100/248): Loss: 0.0112478249 Loss1: 0.0071449541 Loss2: 0.0041028704\n",
      "===> Epoch[132](150/248): Loss: 0.0118468329 Loss1: 0.0071764700 Loss2: 0.0046703625\n",
      "===> Epoch[132](200/248): Loss: 0.0102358516 Loss1: 0.0058501028 Loss2: 0.0043857493\n",
      "PSNR = 37.287\n",
      "Epoch = 133, lr = 2.5e-05\n",
      "===> Epoch[133](50/248): Loss: 0.0127543705 Loss1: 0.0077915168 Loss2: 0.0049628536\n",
      "===> Epoch[133](100/248): Loss: 0.0115674203 Loss1: 0.0072722789 Loss2: 0.0042951414\n",
      "===> Epoch[133](150/248): Loss: 0.0110385362 Loss1: 0.0064763934 Loss2: 0.0045621432\n",
      "===> Epoch[133](200/248): Loss: 0.0093328878 Loss1: 0.0052112052 Loss2: 0.0041216831\n",
      "PSNR = 37.217\n",
      "Epoch = 134, lr = 2.5e-05\n",
      "===> Epoch[134](50/248): Loss: 0.0109573863 Loss1: 0.0066289301 Loss2: 0.0043284567\n",
      "===> Epoch[134](100/248): Loss: 0.0108522568 Loss1: 0.0062859007 Loss2: 0.0045663561\n",
      "===> Epoch[134](150/248): Loss: 0.0097269733 Loss1: 0.0052413889 Loss2: 0.0044855843\n",
      "===> Epoch[134](200/248): Loss: 0.0148903504 Loss1: 0.0097403321 Loss2: 0.0051500187\n",
      "PSNR = 37.276\n",
      "Epoch = 135, lr = 2.5e-05\n",
      "===> Epoch[135](50/248): Loss: 0.0106621943 Loss1: 0.0060442761 Loss2: 0.0046179183\n",
      "===> Epoch[135](100/248): Loss: 0.0108357612 Loss1: 0.0067878412 Loss2: 0.0040479205\n",
      "===> Epoch[135](150/248): Loss: 0.0120385829 Loss1: 0.0071933572 Loss2: 0.0048452257\n",
      "===> Epoch[135](200/248): Loss: 0.0112423971 Loss1: 0.0071080718 Loss2: 0.0041343258\n",
      "PSNR = 37.213\n",
      "Checkpoint saved to ./checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_135.pth\n",
      "Epoch = 136, lr = 2.5e-05\n",
      "===> Epoch[136](50/248): Loss: 0.0126576945 Loss1: 0.0079972316 Loss2: 0.0046604625\n",
      "===> Epoch[136](100/248): Loss: 0.0119884945 Loss1: 0.0069463109 Loss2: 0.0050421837\n",
      "===> Epoch[136](150/248): Loss: 0.0154557917 Loss1: 0.0100199580 Loss2: 0.0054358332\n",
      "===> Epoch[136](200/248): Loss: 0.0149798710 Loss1: 0.0101371305 Loss2: 0.0048427409\n",
      "PSNR = 37.272\n",
      "Epoch = 137, lr = 2.5e-05\n",
      "===> Epoch[137](50/248): Loss: 0.0105438400 Loss1: 0.0059367507 Loss2: 0.0046070898\n",
      "===> Epoch[137](100/248): Loss: 0.0097726453 Loss1: 0.0057240548 Loss2: 0.0040485905\n",
      "===> Epoch[137](150/248): Loss: 0.0115533192 Loss1: 0.0072090509 Loss2: 0.0043442678\n",
      "===> Epoch[137](200/248): Loss: 0.0105849812 Loss1: 0.0059934468 Loss2: 0.0045915344\n",
      "PSNR = 37.306\n",
      "Epoch = 138, lr = 2.5e-05\n",
      "===> Epoch[138](50/248): Loss: 0.0114165312 Loss1: 0.0074819368 Loss2: 0.0039345943\n",
      "===> Epoch[138](100/248): Loss: 0.0107481470 Loss1: 0.0062602968 Loss2: 0.0044878502\n",
      "===> Epoch[138](150/248): Loss: 0.0101956399 Loss1: 0.0059756944 Loss2: 0.0042199455\n",
      "===> Epoch[138](200/248): Loss: 0.0086130127 Loss1: 0.0051589827 Loss2: 0.0034540300\n",
      "PSNR = 37.208\n",
      "Epoch = 139, lr = 2.5e-05\n",
      "===> Epoch[139](50/248): Loss: 0.0094909891 Loss1: 0.0053898506 Loss2: 0.0041011386\n",
      "===> Epoch[139](100/248): Loss: 0.0112281740 Loss1: 0.0071561285 Loss2: 0.0040720450\n",
      "===> Epoch[139](150/248): Loss: 0.0134732891 Loss1: 0.0087621342 Loss2: 0.0047111544\n",
      "===> Epoch[139](200/248): Loss: 0.0092741400 Loss1: 0.0052024741 Loss2: 0.0040716655\n",
      "PSNR = 37.240\n",
      "Epoch = 140, lr = 2.5e-05\n",
      "===> Epoch[140](50/248): Loss: 0.0151303969 Loss1: 0.0099016400 Loss2: 0.0052287574\n",
      "===> Epoch[140](100/248): Loss: 0.0125262905 Loss1: 0.0082654664 Loss2: 0.0042608245\n",
      "===> Epoch[140](150/248): Loss: 0.0148092024 Loss1: 0.0094435057 Loss2: 0.0053656967\n",
      "===> Epoch[140](200/248): Loss: 0.0108603910 Loss1: 0.0061691059 Loss2: 0.0046912855\n",
      "PSNR = 37.209\n",
      "Checkpoint saved to ./checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_140.pth\n",
      "Epoch = 141, lr = 2.5e-05\n",
      "===> Epoch[141](50/248): Loss: 0.0125796776 Loss1: 0.0075535886 Loss2: 0.0050260886\n",
      "===> Epoch[141](100/248): Loss: 0.0108378977 Loss1: 0.0071343211 Loss2: 0.0037035763\n",
      "===> Epoch[141](150/248): Loss: 0.0114641842 Loss1: 0.0072325021 Loss2: 0.0042316816\n",
      "===> Epoch[141](200/248): Loss: 0.0103010498 Loss1: 0.0058998275 Loss2: 0.0044012223\n",
      "PSNR = 37.261\n",
      "Epoch = 142, lr = 2.5e-05\n",
      "===> Epoch[142](50/248): Loss: 0.0102557400 Loss1: 0.0062701716 Loss2: 0.0039855684\n",
      "===> Epoch[142](100/248): Loss: 0.0122665735 Loss1: 0.0076948763 Loss2: 0.0045716972\n",
      "===> Epoch[142](150/248): Loss: 0.0125265568 Loss1: 0.0082909400 Loss2: 0.0042356169\n",
      "===> Epoch[142](200/248): Loss: 0.0134533979 Loss1: 0.0085576158 Loss2: 0.0048957826\n",
      "PSNR = 37.280\n",
      "Epoch = 143, lr = 2.5e-05\n",
      "===> Epoch[143](50/248): Loss: 0.0100766066 Loss1: 0.0061313007 Loss2: 0.0039453055\n",
      "===> Epoch[143](100/248): Loss: 0.0095732249 Loss1: 0.0058155661 Loss2: 0.0037576591\n",
      "===> Epoch[143](150/248): Loss: 0.0134123983 Loss1: 0.0087407678 Loss2: 0.0046716304\n",
      "===> Epoch[143](200/248): Loss: 0.0115001909 Loss1: 0.0073772562 Loss2: 0.0041229348\n",
      "PSNR = 37.262\n",
      "Epoch = 144, lr = 2.5e-05\n",
      "===> Epoch[144](50/248): Loss: 0.0105453525 Loss1: 0.0059594354 Loss2: 0.0045859166\n",
      "===> Epoch[144](100/248): Loss: 0.0116240885 Loss1: 0.0071878191 Loss2: 0.0044362694\n",
      "===> Epoch[144](150/248): Loss: 0.0127454074 Loss1: 0.0078956001 Loss2: 0.0048498074\n",
      "===> Epoch[144](200/248): Loss: 0.0128008965 Loss1: 0.0075371126 Loss2: 0.0052637840\n",
      "PSNR = 37.262\n",
      "Epoch = 145, lr = 2.5e-05\n",
      "===> Epoch[145](50/248): Loss: 0.0114832744 Loss1: 0.0069697755 Loss2: 0.0045134993\n",
      "===> Epoch[145](100/248): Loss: 0.0122810509 Loss1: 0.0074303434 Loss2: 0.0048507075\n",
      "===> Epoch[145](150/248): Loss: 0.0105717294 Loss1: 0.0058071488 Loss2: 0.0047645806\n",
      "===> Epoch[145](200/248): Loss: 0.0119330296 Loss1: 0.0073711164 Loss2: 0.0045619132\n",
      "PSNR = 37.290\n",
      "Checkpoint saved to ./checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_145.pth\n",
      "Epoch = 146, lr = 2.5e-05\n",
      "===> Epoch[146](50/248): Loss: 0.0091529191 Loss1: 0.0055021895 Loss2: 0.0036507291\n",
      "===> Epoch[146](100/248): Loss: 0.0102658551 Loss1: 0.0059883506 Loss2: 0.0042775040\n",
      "===> Epoch[146](150/248): Loss: 0.0106985066 Loss1: 0.0067851357 Loss2: 0.0039133714\n",
      "===> Epoch[146](200/248): Loss: 0.0115436725 Loss1: 0.0068233158 Loss2: 0.0047203563\n",
      "PSNR = 37.252\n",
      "Epoch = 147, lr = 2.5e-05\n",
      "===> Epoch[147](50/248): Loss: 0.0114863198 Loss1: 0.0067550731 Loss2: 0.0047312463\n",
      "===> Epoch[147](100/248): Loss: 0.0108191818 Loss1: 0.0064681536 Loss2: 0.0043510287\n",
      "===> Epoch[147](150/248): Loss: 0.0110580418 Loss1: 0.0064061414 Loss2: 0.0046518999\n",
      "===> Epoch[147](200/248): Loss: 0.0116171353 Loss1: 0.0069993446 Loss2: 0.0046177912\n",
      "PSNR = 37.300\n",
      "Epoch = 148, lr = 2.5e-05\n",
      "===> Epoch[148](50/248): Loss: 0.0109944902 Loss1: 0.0068770270 Loss2: 0.0041174637\n",
      "===> Epoch[148](100/248): Loss: 0.0106633995 Loss1: 0.0064933845 Loss2: 0.0041700150\n",
      "===> Epoch[148](150/248): Loss: 0.0123586357 Loss1: 0.0079000536 Loss2: 0.0044585816\n",
      "===> Epoch[148](200/248): Loss: 0.0134507064 Loss1: 0.0082171503 Loss2: 0.0052335556\n",
      "PSNR = 37.308\n",
      "Epoch = 149, lr = 2.5e-05\n",
      "===> Epoch[149](50/248): Loss: 0.0131172873 Loss1: 0.0086122556 Loss2: 0.0045050322\n",
      "===> Epoch[149](100/248): Loss: 0.0120123075 Loss1: 0.0076554357 Loss2: 0.0043568718\n",
      "===> Epoch[149](150/248): Loss: 0.0107109826 Loss1: 0.0060611046 Loss2: 0.0046498785\n",
      "===> Epoch[149](200/248): Loss: 0.0102604255 Loss1: 0.0058293822 Loss2: 0.0044310438\n",
      "PSNR = 37.315\n",
      "Epoch = 150, lr = 2.5e-05\n",
      "===> Epoch[150](50/248): Loss: 0.0125262411 Loss1: 0.0079373419 Loss2: 0.0045888992\n",
      "===> Epoch[150](100/248): Loss: 0.0117668156 Loss1: 0.0071676956 Loss2: 0.0045991200\n",
      "===> Epoch[150](150/248): Loss: 0.0150778182 Loss1: 0.0098685808 Loss2: 0.0052092369\n",
      "===> Epoch[150](200/248): Loss: 0.0109688425 Loss1: 0.0065570693 Loss2: 0.0044117733\n",
      "PSNR = 37.275\n",
      "Checkpoint saved to ./checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_150.pth\n",
      "Epoch = 151, lr = 2.5e-05\n",
      "===> Epoch[151](50/248): Loss: 0.0101526100 Loss1: 0.0060408195 Loss2: 0.0041117906\n",
      "===> Epoch[151](100/248): Loss: 0.0113657527 Loss1: 0.0074773026 Loss2: 0.0038884506\n",
      "===> Epoch[151](150/248): Loss: 0.0094823064 Loss1: 0.0058912500 Loss2: 0.0035910562\n",
      "===> Epoch[151](200/248): Loss: 0.0102905892 Loss1: 0.0061971298 Loss2: 0.0040934593\n",
      "PSNR = 37.245\n",
      "Epoch = 152, lr = 2.5e-05\n",
      "===> Epoch[152](50/248): Loss: 0.0108198635 Loss1: 0.0067056213 Loss2: 0.0041142427\n",
      "===> Epoch[152](100/248): Loss: 0.0118450075 Loss1: 0.0073907864 Loss2: 0.0044542211\n",
      "===> Epoch[152](150/248): Loss: 0.0103867594 Loss1: 0.0063129952 Loss2: 0.0040737647\n",
      "===> Epoch[152](200/248): Loss: 0.0089019965 Loss1: 0.0050706291 Loss2: 0.0038313677\n",
      "PSNR = 37.262\n",
      "Epoch = 153, lr = 2.5e-05\n",
      "===> Epoch[153](50/248): Loss: 0.0144373327 Loss1: 0.0088667432 Loss2: 0.0055705900\n",
      "===> Epoch[153](100/248): Loss: 0.0102538690 Loss1: 0.0067004454 Loss2: 0.0035534233\n",
      "===> Epoch[153](150/248): Loss: 0.0107550798 Loss1: 0.0067476886 Loss2: 0.0040073912\n",
      "===> Epoch[153](200/248): Loss: 0.0109670106 Loss1: 0.0067812377 Loss2: 0.0041857730\n",
      "PSNR = 37.296\n",
      "Epoch = 154, lr = 2.5e-05\n",
      "===> Epoch[154](50/248): Loss: 0.0109390393 Loss1: 0.0067163161 Loss2: 0.0042227232\n",
      "===> Epoch[154](100/248): Loss: 0.0138816796 Loss1: 0.0087642148 Loss2: 0.0051174643\n",
      "===> Epoch[154](150/248): Loss: 0.0103253182 Loss1: 0.0062651527 Loss2: 0.0040601655\n",
      "===> Epoch[154](200/248): Loss: 0.0116708055 Loss1: 0.0071358611 Loss2: 0.0045349444\n",
      "PSNR = 37.257\n",
      "Epoch = 155, lr = 2.5e-05\n",
      "===> Epoch[155](50/248): Loss: 0.0116126258 Loss1: 0.0074485010 Loss2: 0.0041641253\n",
      "===> Epoch[155](100/248): Loss: 0.0101340711 Loss1: 0.0061100125 Loss2: 0.0040240590\n",
      "===> Epoch[155](150/248): Loss: 0.0111530069 Loss1: 0.0069078114 Loss2: 0.0042451951\n",
      "===> Epoch[155](200/248): Loss: 0.0144803096 Loss1: 0.0093689058 Loss2: 0.0051114038\n",
      "PSNR = 37.284\n",
      "Checkpoint saved to ./checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_155.pth\n",
      "Epoch = 156, lr = 1.25e-05\n",
      "===> Epoch[156](50/248): Loss: 0.0137318289 Loss1: 0.0087354435 Loss2: 0.0049963854\n",
      "===> Epoch[156](100/248): Loss: 0.0131299011 Loss1: 0.0085147237 Loss2: 0.0046151779\n",
      "===> Epoch[156](150/248): Loss: 0.0096511338 Loss1: 0.0051625185 Loss2: 0.0044886149\n",
      "===> Epoch[156](200/248): Loss: 0.0133382697 Loss1: 0.0088775866 Loss2: 0.0044606831\n",
      "PSNR = 37.277\n",
      "Epoch = 157, lr = 1.25e-05\n",
      "===> Epoch[157](50/248): Loss: 0.0101872599 Loss1: 0.0058265938 Loss2: 0.0043606660\n",
      "===> Epoch[157](100/248): Loss: 0.0093566040 Loss1: 0.0055102562 Loss2: 0.0038463478\n",
      "===> Epoch[157](150/248): Loss: 0.0115438225 Loss1: 0.0073269363 Loss2: 0.0042168861\n",
      "===> Epoch[157](200/248): Loss: 0.0108036324 Loss1: 0.0062704091 Loss2: 0.0045332238\n",
      "PSNR = 37.279\n",
      "Epoch = 158, lr = 1.25e-05\n",
      "===> Epoch[158](50/248): Loss: 0.0121578630 Loss1: 0.0072803702 Loss2: 0.0048774928\n",
      "===> Epoch[158](100/248): Loss: 0.0108767832 Loss1: 0.0066152364 Loss2: 0.0042615468\n",
      "===> Epoch[158](150/248): Loss: 0.0094305808 Loss1: 0.0056949938 Loss2: 0.0037355868\n",
      "===> Epoch[158](200/248): Loss: 0.0125351474 Loss1: 0.0079324972 Loss2: 0.0046026506\n",
      "PSNR = 37.293\n",
      "Epoch = 159, lr = 1.25e-05\n",
      "===> Epoch[159](50/248): Loss: 0.0097431708 Loss1: 0.0056845611 Loss2: 0.0040586097\n",
      "===> Epoch[159](100/248): Loss: 0.0104760882 Loss1: 0.0063599758 Loss2: 0.0041161128\n",
      "===> Epoch[159](150/248): Loss: 0.0098096263 Loss1: 0.0062611294 Loss2: 0.0035484971\n",
      "===> Epoch[159](200/248): Loss: 0.0101734940 Loss1: 0.0061027235 Loss2: 0.0040707705\n",
      "PSNR = 37.323\n",
      "Epoch = 160, lr = 1.25e-05\n",
      "===> Epoch[160](50/248): Loss: 0.0105757331 Loss1: 0.0062423567 Loss2: 0.0043333764\n",
      "===> Epoch[160](100/248): Loss: 0.0133300675 Loss1: 0.0084124068 Loss2: 0.0049176607\n",
      "===> Epoch[160](150/248): Loss: 0.0106507931 Loss1: 0.0064240741 Loss2: 0.0042267195\n",
      "===> Epoch[160](200/248): Loss: 0.0096375002 Loss1: 0.0060250005 Loss2: 0.0036124997\n",
      "PSNR = 37.261\n",
      "Checkpoint saved to ./checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_160.pth\n",
      "Epoch = 161, lr = 1.25e-05\n",
      "===> Epoch[161](50/248): Loss: 0.0108114900 Loss1: 0.0061983555 Loss2: 0.0046131345\n",
      "===> Epoch[161](100/248): Loss: 0.0125249419 Loss1: 0.0073687294 Loss2: 0.0051562130\n",
      "===> Epoch[161](150/248): Loss: 0.0118492274 Loss1: 0.0073487437 Loss2: 0.0045004836\n",
      "===> Epoch[161](200/248): Loss: 0.0133303935 Loss1: 0.0084463628 Loss2: 0.0048840307\n",
      "PSNR = 37.272\n",
      "Epoch = 162, lr = 1.25e-05\n",
      "===> Epoch[162](50/248): Loss: 0.0100834854 Loss1: 0.0058174185 Loss2: 0.0042660674\n",
      "===> Epoch[162](100/248): Loss: 0.0107594933 Loss1: 0.0066416492 Loss2: 0.0041178437\n",
      "===> Epoch[162](150/248): Loss: 0.0109190708 Loss1: 0.0063791191 Loss2: 0.0045399517\n",
      "===> Epoch[162](200/248): Loss: 0.0103940181 Loss1: 0.0063171382 Loss2: 0.0040768795\n",
      "PSNR = 37.292\n",
      "Epoch = 163, lr = 1.25e-05\n",
      "===> Epoch[163](50/248): Loss: 0.0097985901 Loss1: 0.0056364709 Loss2: 0.0041621197\n",
      "===> Epoch[163](100/248): Loss: 0.0125675872 Loss1: 0.0076757274 Loss2: 0.0048918598\n",
      "===> Epoch[163](150/248): Loss: 0.0134380311 Loss1: 0.0086765150 Loss2: 0.0047615157\n",
      "===> Epoch[163](200/248): Loss: 0.0121724224 Loss1: 0.0075811287 Loss2: 0.0045912936\n",
      "PSNR = 37.272\n",
      "Epoch = 164, lr = 1.25e-05\n",
      "===> Epoch[164](50/248): Loss: 0.0097905323 Loss1: 0.0055295229 Loss2: 0.0042610094\n",
      "===> Epoch[164](100/248): Loss: 0.0119353328 Loss1: 0.0076593878 Loss2: 0.0042759450\n",
      "===> Epoch[164](150/248): Loss: 0.0102181043 Loss1: 0.0060486463 Loss2: 0.0041694581\n",
      "===> Epoch[164](200/248): Loss: 0.0121839307 Loss1: 0.0074605360 Loss2: 0.0047233943\n",
      "PSNR = 37.269\n",
      "Epoch = 165, lr = 1.25e-05\n",
      "===> Epoch[165](50/248): Loss: 0.0100492444 Loss1: 0.0062010959 Loss2: 0.0038481490\n",
      "===> Epoch[165](100/248): Loss: 0.0089438800 Loss1: 0.0049443124 Loss2: 0.0039995671\n",
      "===> Epoch[165](150/248): Loss: 0.0109928474 Loss1: 0.0067779454 Loss2: 0.0042149015\n",
      "===> Epoch[165](200/248): Loss: 0.0138562638 Loss1: 0.0087173283 Loss2: 0.0051389355\n",
      "PSNR = 37.254\n",
      "Checkpoint saved to ./checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_165.pth\n",
      "Epoch = 166, lr = 1.25e-05\n",
      "===> Epoch[166](50/248): Loss: 0.0120333135 Loss1: 0.0076322109 Loss2: 0.0044011027\n",
      "===> Epoch[166](100/248): Loss: 0.0095935427 Loss1: 0.0052448865 Loss2: 0.0043486566\n",
      "===> Epoch[166](150/248): Loss: 0.0095596239 Loss1: 0.0056931251 Loss2: 0.0038664991\n",
      "===> Epoch[166](200/248): Loss: 0.0125702284 Loss1: 0.0078985477 Loss2: 0.0046716812\n",
      "PSNR = 37.288\n",
      "Epoch = 167, lr = 1.25e-05\n",
      "===> Epoch[167](50/248): Loss: 0.0100949043 Loss1: 0.0060303616 Loss2: 0.0040645427\n",
      "===> Epoch[167](100/248): Loss: 0.0075474828 Loss1: 0.0038430279 Loss2: 0.0037044547\n",
      "===> Epoch[167](150/248): Loss: 0.0135777090 Loss1: 0.0090432549 Loss2: 0.0045344546\n",
      "===> Epoch[167](200/248): Loss: 0.0138461581 Loss1: 0.0093181087 Loss2: 0.0045280494\n",
      "PSNR = 37.264\n",
      "Epoch = 168, lr = 1.25e-05\n",
      "===> Epoch[168](50/248): Loss: 0.0116108386 Loss1: 0.0068320832 Loss2: 0.0047787554\n",
      "===> Epoch[168](100/248): Loss: 0.0115911253 Loss1: 0.0072341110 Loss2: 0.0043570139\n",
      "===> Epoch[168](150/248): Loss: 0.0127642397 Loss1: 0.0079027507 Loss2: 0.0048614885\n",
      "===> Epoch[168](200/248): Loss: 0.0111532528 Loss1: 0.0067701694 Loss2: 0.0043830830\n",
      "PSNR = 37.318\n",
      "Epoch = 169, lr = 1.25e-05\n",
      "===> Epoch[169](50/248): Loss: 0.0097866785 Loss1: 0.0054835468 Loss2: 0.0043031317\n",
      "===> Epoch[169](100/248): Loss: 0.0124993790 Loss1: 0.0076635699 Loss2: 0.0048358086\n",
      "===> Epoch[169](150/248): Loss: 0.0116461944 Loss1: 0.0067588529 Loss2: 0.0048873411\n",
      "===> Epoch[169](200/248): Loss: 0.0148539711 Loss1: 0.0098993797 Loss2: 0.0049545914\n",
      "PSNR = 37.291\n",
      "Epoch = 170, lr = 1.25e-05\n",
      "===> Epoch[170](50/248): Loss: 0.0087719169 Loss1: 0.0051307529 Loss2: 0.0036411637\n",
      "===> Epoch[170](100/248): Loss: 0.0102758296 Loss1: 0.0060914811 Loss2: 0.0041843480\n",
      "===> Epoch[170](150/248): Loss: 0.0117494259 Loss1: 0.0070327460 Loss2: 0.0047166795\n",
      "===> Epoch[170](200/248): Loss: 0.0131012006 Loss1: 0.0083969692 Loss2: 0.0047042314\n",
      "PSNR = 37.287\n",
      "Checkpoint saved to ./checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_170.pth\n",
      "Epoch = 171, lr = 1.25e-05\n",
      "===> Epoch[171](50/248): Loss: 0.0111594610 Loss1: 0.0068792794 Loss2: 0.0042801811\n",
      "===> Epoch[171](100/248): Loss: 0.0133838393 Loss1: 0.0087058870 Loss2: 0.0046779523\n",
      "===> Epoch[171](150/248): Loss: 0.0126748160 Loss1: 0.0083152466 Loss2: 0.0043595689\n",
      "===> Epoch[171](200/248): Loss: 0.0109165749 Loss1: 0.0066780997 Loss2: 0.0042384751\n",
      "PSNR = 37.270\n",
      "Epoch = 172, lr = 1.25e-05\n",
      "===> Epoch[172](50/248): Loss: 0.0109758191 Loss1: 0.0069767986 Loss2: 0.0039990209\n",
      "===> Epoch[172](100/248): Loss: 0.0125982836 Loss1: 0.0084038796 Loss2: 0.0041944045\n",
      "===> Epoch[172](150/248): Loss: 0.0090227425 Loss1: 0.0056601842 Loss2: 0.0033625581\n",
      "===> Epoch[172](200/248): Loss: 0.0116712609 Loss1: 0.0072940006 Loss2: 0.0043772603\n",
      "PSNR = 37.301\n",
      "Epoch = 173, lr = 1.25e-05\n",
      "===> Epoch[173](50/248): Loss: 0.0107304901 Loss1: 0.0063752481 Loss2: 0.0043552420\n",
      "===> Epoch[173](100/248): Loss: 0.0113132633 Loss1: 0.0067925844 Loss2: 0.0045206794\n",
      "===> Epoch[173](150/248): Loss: 0.0100580212 Loss1: 0.0060006967 Loss2: 0.0040573250\n",
      "===> Epoch[173](200/248): Loss: 0.0096796108 Loss1: 0.0054958076 Loss2: 0.0041838032\n",
      "PSNR = 37.285\n",
      "Epoch = 174, lr = 1.25e-05\n",
      "===> Epoch[174](50/248): Loss: 0.0100031979 Loss1: 0.0061109858 Loss2: 0.0038922117\n",
      "===> Epoch[174](100/248): Loss: 0.0089476313 Loss1: 0.0052463776 Loss2: 0.0037012533\n",
      "===> Epoch[174](150/248): Loss: 0.0132078622 Loss1: 0.0083141094 Loss2: 0.0048937523\n",
      "===> Epoch[174](200/248): Loss: 0.0094629172 Loss1: 0.0052352138 Loss2: 0.0042277039\n",
      "PSNR = 37.293\n",
      "Epoch = 175, lr = 1.25e-05\n",
      "===> Epoch[175](50/248): Loss: 0.0103426576 Loss1: 0.0059302524 Loss2: 0.0044124047\n",
      "===> Epoch[175](100/248): Loss: 0.0106989108 Loss1: 0.0069286139 Loss2: 0.0037702972\n",
      "===> Epoch[175](150/248): Loss: 0.0108751673 Loss1: 0.0068220766 Loss2: 0.0040530912\n",
      "===> Epoch[175](200/248): Loss: 0.0154803395 Loss1: 0.0103323078 Loss2: 0.0051480317\n",
      "PSNR = 37.298\n",
      "Checkpoint saved to ./checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_175.pth\n",
      "Epoch = 176, lr = 1.25e-05\n",
      "===> Epoch[176](50/248): Loss: 0.0109686088 Loss1: 0.0069552236 Loss2: 0.0040133852\n",
      "===> Epoch[176](100/248): Loss: 0.0103443014 Loss1: 0.0059796302 Loss2: 0.0043646712\n",
      "===> Epoch[176](150/248): Loss: 0.0114585925 Loss1: 0.0073592747 Loss2: 0.0040993183\n",
      "===> Epoch[176](200/248): Loss: 0.0098331161 Loss1: 0.0059029167 Loss2: 0.0039301990\n",
      "PSNR = 37.282\n",
      "Epoch = 177, lr = 1.25e-05\n",
      "===> Epoch[177](50/248): Loss: 0.0114077926 Loss1: 0.0070232847 Loss2: 0.0043845084\n",
      "===> Epoch[177](100/248): Loss: 0.0099686161 Loss1: 0.0060172705 Loss2: 0.0039513460\n",
      "===> Epoch[177](150/248): Loss: 0.0081558116 Loss1: 0.0045837546 Loss2: 0.0035720572\n",
      "===> Epoch[177](200/248): Loss: 0.0127377613 Loss1: 0.0081461845 Loss2: 0.0045915768\n",
      "PSNR = 37.291\n",
      "Epoch = 178, lr = 1.25e-05\n",
      "===> Epoch[178](50/248): Loss: 0.0140433442 Loss1: 0.0088368058 Loss2: 0.0052065379\n",
      "===> Epoch[178](100/248): Loss: 0.0119032701 Loss1: 0.0069776457 Loss2: 0.0049256245\n",
      "===> Epoch[178](150/248): Loss: 0.0120597063 Loss1: 0.0078145163 Loss2: 0.0042451899\n",
      "===> Epoch[178](200/248): Loss: 0.0108352760 Loss1: 0.0066691255 Loss2: 0.0041661505\n",
      "PSNR = 37.277\n",
      "Epoch = 179, lr = 1.25e-05\n",
      "===> Epoch[179](50/248): Loss: 0.0101713073 Loss1: 0.0058142198 Loss2: 0.0043570870\n",
      "===> Epoch[179](100/248): Loss: 0.0101980176 Loss1: 0.0062025082 Loss2: 0.0039955098\n",
      "===> Epoch[179](150/248): Loss: 0.0128160007 Loss1: 0.0078354068 Loss2: 0.0049805935\n",
      "===> Epoch[179](200/248): Loss: 0.0123603623 Loss1: 0.0080643995 Loss2: 0.0042959633\n",
      "PSNR = 37.266\n",
      "Epoch = 180, lr = 1.25e-05\n",
      "===> Epoch[180](50/248): Loss: 0.0105892885 Loss1: 0.0063137333 Loss2: 0.0042755553\n",
      "===> Epoch[180](100/248): Loss: 0.0108397026 Loss1: 0.0069013247 Loss2: 0.0039383783\n",
      "===> Epoch[180](150/248): Loss: 0.0113533568 Loss1: 0.0070996955 Loss2: 0.0042536617\n",
      "===> Epoch[180](200/248): Loss: 0.0107558705 Loss1: 0.0065114805 Loss2: 0.0042443904\n",
      "PSNR = 37.277\n",
      "Checkpoint saved to ./checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_180.pth\n",
      "Epoch = 181, lr = 1.25e-05\n",
      "===> Epoch[181](50/248): Loss: 0.0126315644 Loss1: 0.0079297367 Loss2: 0.0047018277\n",
      "===> Epoch[181](100/248): Loss: 0.0098636504 Loss1: 0.0051890202 Loss2: 0.0046746307\n",
      "===> Epoch[181](150/248): Loss: 0.0096553164 Loss1: 0.0058575291 Loss2: 0.0037977875\n",
      "===> Epoch[181](200/248): Loss: 0.0107024536 Loss1: 0.0062589939 Loss2: 0.0044434597\n",
      "PSNR = 37.293\n",
      "Epoch = 182, lr = 1.25e-05\n",
      "===> Epoch[182](50/248): Loss: 0.0094635710 Loss1: 0.0056543103 Loss2: 0.0038092609\n",
      "===> Epoch[182](100/248): Loss: 0.0106300768 Loss1: 0.0063197743 Loss2: 0.0043103020\n",
      "===> Epoch[182](150/248): Loss: 0.0131251710 Loss1: 0.0084177852 Loss2: 0.0047073858\n",
      "===> Epoch[182](200/248): Loss: 0.0114129577 Loss1: 0.0068088043 Loss2: 0.0046041533\n",
      "PSNR = 37.285\n",
      "Epoch = 183, lr = 1.25e-05\n",
      "===> Epoch[183](50/248): Loss: 0.0115368972 Loss1: 0.0071746274 Loss2: 0.0043622698\n",
      "===> Epoch[183](100/248): Loss: 0.0132902451 Loss1: 0.0088977711 Loss2: 0.0043924735\n",
      "===> Epoch[183](150/248): Loss: 0.0111022359 Loss1: 0.0069148210 Loss2: 0.0041874149\n",
      "===> Epoch[183](200/248): Loss: 0.0104239322 Loss1: 0.0061806948 Loss2: 0.0042432374\n",
      "PSNR = 37.268\n",
      "Epoch = 184, lr = 1.25e-05\n",
      "===> Epoch[184](50/248): Loss: 0.0116189588 Loss1: 0.0074932706 Loss2: 0.0041256882\n",
      "===> Epoch[184](100/248): Loss: 0.0108090043 Loss1: 0.0064908350 Loss2: 0.0043181698\n",
      "===> Epoch[184](150/248): Loss: 0.0132135004 Loss1: 0.0080003180 Loss2: 0.0052131829\n",
      "===> Epoch[184](200/248): Loss: 0.0105429655 Loss1: 0.0062662312 Loss2: 0.0042767343\n",
      "PSNR = 37.254\n",
      "Epoch = 185, lr = 1.25e-05\n",
      "===> Epoch[185](50/248): Loss: 0.0102535877 Loss1: 0.0060359477 Loss2: 0.0042176396\n",
      "===> Epoch[185](100/248): Loss: 0.0098019093 Loss1: 0.0059484160 Loss2: 0.0038534936\n",
      "===> Epoch[185](150/248): Loss: 0.0090230433 Loss1: 0.0044389139 Loss2: 0.0045841294\n",
      "===> Epoch[185](200/248): Loss: 0.0124083366 Loss1: 0.0076235714 Loss2: 0.0047847657\n",
      "PSNR = 37.310\n",
      "Checkpoint saved to ./checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_185.pth\n",
      "Epoch = 186, lr = 1.25e-05\n",
      "===> Epoch[186](50/248): Loss: 0.0137911085 Loss1: 0.0090073347 Loss2: 0.0047837733\n",
      "===> Epoch[186](100/248): Loss: 0.0109190010 Loss1: 0.0064621400 Loss2: 0.0044568609\n",
      "===> Epoch[186](150/248): Loss: 0.0108495299 Loss1: 0.0067758528 Loss2: 0.0040736767\n",
      "===> Epoch[186](200/248): Loss: 0.0134815257 Loss1: 0.0085890954 Loss2: 0.0048924307\n",
      "PSNR = 37.304\n",
      "Epoch = 187, lr = 1.25e-05\n",
      "===> Epoch[187](50/248): Loss: 0.0115071395 Loss1: 0.0073876395 Loss2: 0.0041195001\n",
      "===> Epoch[187](100/248): Loss: 0.0123237930 Loss1: 0.0078769727 Loss2: 0.0044468204\n",
      "===> Epoch[187](150/248): Loss: 0.0101527907 Loss1: 0.0056853816 Loss2: 0.0044674091\n",
      "===> Epoch[187](200/248): Loss: 0.0134232305 Loss1: 0.0087712090 Loss2: 0.0046520219\n",
      "PSNR = 37.283\n",
      "Epoch = 188, lr = 1.25e-05\n",
      "===> Epoch[188](50/248): Loss: 0.0090434663 Loss1: 0.0049512866 Loss2: 0.0040921792\n",
      "===> Epoch[188](100/248): Loss: 0.0124978442 Loss1: 0.0075603276 Loss2: 0.0049375161\n",
      "===> Epoch[188](150/248): Loss: 0.0092008598 Loss1: 0.0054547051 Loss2: 0.0037461545\n",
      "===> Epoch[188](200/248): Loss: 0.0103686079 Loss1: 0.0063199773 Loss2: 0.0040486306\n",
      "PSNR = 37.276\n",
      "Epoch = 189, lr = 1.25e-05\n",
      "===> Epoch[189](50/248): Loss: 0.0097820768 Loss1: 0.0057270341 Loss2: 0.0040550428\n",
      "===> Epoch[189](100/248): Loss: 0.0125053767 Loss1: 0.0082810111 Loss2: 0.0042243660\n",
      "===> Epoch[189](150/248): Loss: 0.0105029047 Loss1: 0.0062608165 Loss2: 0.0042420882\n",
      "===> Epoch[189](200/248): Loss: 0.0104375305 Loss1: 0.0064282725 Loss2: 0.0040092580\n",
      "PSNR = 37.269\n",
      "Epoch = 190, lr = 1.25e-05\n",
      "===> Epoch[190](50/248): Loss: 0.0098481067 Loss1: 0.0058393688 Loss2: 0.0040087379\n",
      "===> Epoch[190](100/248): Loss: 0.0122697428 Loss1: 0.0075154821 Loss2: 0.0047542606\n",
      "===> Epoch[190](150/248): Loss: 0.0110750478 Loss1: 0.0069402088 Loss2: 0.0041348385\n",
      "===> Epoch[190](200/248): Loss: 0.0110912286 Loss1: 0.0067830575 Loss2: 0.0043081711\n",
      "PSNR = 37.271\n",
      "Checkpoint saved to ./checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_190.pth\n",
      "Epoch = 191, lr = 6.25e-06\n",
      "===> Epoch[191](50/248): Loss: 0.0103541669 Loss1: 0.0064204931 Loss2: 0.0039336742\n",
      "===> Epoch[191](100/248): Loss: 0.0084590772 Loss1: 0.0049531576 Loss2: 0.0035059198\n",
      "===> Epoch[191](150/248): Loss: 0.0105098672 Loss1: 0.0057693198 Loss2: 0.0047405469\n",
      "===> Epoch[191](200/248): Loss: 0.0135807060 Loss1: 0.0087653920 Loss2: 0.0048153135\n",
      "PSNR = 37.299\n",
      "Epoch = 192, lr = 6.25e-06\n",
      "===> Epoch[192](50/248): Loss: 0.0091167288 Loss1: 0.0051980936 Loss2: 0.0039186352\n",
      "===> Epoch[192](100/248): Loss: 0.0097751925 Loss1: 0.0059224046 Loss2: 0.0038527877\n",
      "===> Epoch[192](150/248): Loss: 0.0102138147 Loss1: 0.0063479296 Loss2: 0.0038658851\n",
      "===> Epoch[192](200/248): Loss: 0.0092987148 Loss1: 0.0054159900 Loss2: 0.0038827248\n",
      "PSNR = 37.286\n",
      "Epoch = 193, lr = 6.25e-06\n",
      "===> Epoch[193](50/248): Loss: 0.0107492497 Loss1: 0.0070241303 Loss2: 0.0037251196\n",
      "===> Epoch[193](100/248): Loss: 0.0099414494 Loss1: 0.0057269903 Loss2: 0.0042144591\n",
      "===> Epoch[193](150/248): Loss: 0.0092100995 Loss1: 0.0059076566 Loss2: 0.0033024431\n",
      "===> Epoch[193](200/248): Loss: 0.0133214276 Loss1: 0.0084122373 Loss2: 0.0049091908\n",
      "PSNR = 37.301\n",
      "Epoch = 194, lr = 6.25e-06\n",
      "===> Epoch[194](50/248): Loss: 0.0101136928 Loss1: 0.0060848198 Loss2: 0.0040288731\n",
      "===> Epoch[194](100/248): Loss: 0.0137563311 Loss1: 0.0089429459 Loss2: 0.0048133847\n",
      "===> Epoch[194](150/248): Loss: 0.0092369476 Loss1: 0.0048828390 Loss2: 0.0043541086\n",
      "===> Epoch[194](200/248): Loss: 0.0101776933 Loss1: 0.0064300671 Loss2: 0.0037476264\n",
      "PSNR = 37.287\n",
      "Epoch = 195, lr = 6.25e-06\n",
      "===> Epoch[195](50/248): Loss: 0.0117479116 Loss1: 0.0073395092 Loss2: 0.0044084024\n",
      "===> Epoch[195](100/248): Loss: 0.0112872850 Loss1: 0.0071613719 Loss2: 0.0041259131\n",
      "===> Epoch[195](150/248): Loss: 0.0097134020 Loss1: 0.0057074358 Loss2: 0.0040059662\n",
      "===> Epoch[195](200/248): Loss: 0.0111638941 Loss1: 0.0071788644 Loss2: 0.0039850296\n",
      "PSNR = 37.275\n",
      "Checkpoint saved to ./checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_195.pth\n",
      "Epoch = 196, lr = 6.25e-06\n",
      "===> Epoch[196](50/248): Loss: 0.0131493732 Loss1: 0.0082459990 Loss2: 0.0049033742\n",
      "===> Epoch[196](100/248): Loss: 0.0099335052 Loss1: 0.0058767442 Loss2: 0.0040567615\n",
      "===> Epoch[196](150/248): Loss: 0.0120762810 Loss1: 0.0079405364 Loss2: 0.0041357442\n",
      "===> Epoch[196](200/248): Loss: 0.0095830727 Loss1: 0.0057628225 Loss2: 0.0038202498\n",
      "PSNR = 37.287\n",
      "Epoch = 197, lr = 6.25e-06\n",
      "===> Epoch[197](50/248): Loss: 0.0103436988 Loss1: 0.0059625176 Loss2: 0.0043811812\n",
      "===> Epoch[197](100/248): Loss: 0.0109761469 Loss1: 0.0064572925 Loss2: 0.0045188549\n",
      "===> Epoch[197](150/248): Loss: 0.0113106146 Loss1: 0.0072262362 Loss2: 0.0040843780\n",
      "===> Epoch[197](200/248): Loss: 0.0109925494 Loss1: 0.0067124590 Loss2: 0.0042800903\n",
      "PSNR = 37.278\n",
      "Epoch = 198, lr = 6.25e-06\n",
      "===> Epoch[198](50/248): Loss: 0.0103653781 Loss1: 0.0062296428 Loss2: 0.0041357349\n",
      "===> Epoch[198](100/248): Loss: 0.0108476160 Loss1: 0.0064905114 Loss2: 0.0043571047\n",
      "===> Epoch[198](150/248): Loss: 0.0089115063 Loss1: 0.0049095671 Loss2: 0.0040019392\n",
      "===> Epoch[198](200/248): Loss: 0.0098257223 Loss1: 0.0058185100 Loss2: 0.0040072124\n",
      "PSNR = 37.280\n",
      "Epoch = 199, lr = 6.25e-06\n",
      "===> Epoch[199](50/248): Loss: 0.0118249226 Loss1: 0.0071068858 Loss2: 0.0047180364\n",
      "===> Epoch[199](100/248): Loss: 0.0104062036 Loss1: 0.0054969769 Loss2: 0.0049092271\n",
      "===> Epoch[199](150/248): Loss: 0.0125853065 Loss1: 0.0079669850 Loss2: 0.0046183220\n",
      "===> Epoch[199](200/248): Loss: 0.0102605075 Loss1: 0.0063471021 Loss2: 0.0039134054\n",
      "PSNR = 37.281\n",
      "Epoch = 200, lr = 6.25e-06\n",
      "===> Epoch[200](50/248): Loss: 0.0114785973 Loss1: 0.0074081561 Loss2: 0.0040704417\n",
      "===> Epoch[200](100/248): Loss: 0.0101678688 Loss1: 0.0063733449 Loss2: 0.0037945241\n",
      "===> Epoch[200](150/248): Loss: 0.0084710252 Loss1: 0.0046828822 Loss2: 0.0037881432\n",
      "===> Epoch[200](200/248): Loss: 0.0116166295 Loss1: 0.0075066872 Loss2: 0.0041099424\n",
      "PSNR = 37.266\n",
      "Checkpoint saved to ./checkpoints/WTSR00/CAVE/4/nfeats128nblock4sub8_34_batch3200/CAVE_WTSR00_ckpt_epoch_200.pth\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    # if opt.show:\n",
    "    #     global writer\n",
    "    #     writer = SummaryWriter(log_dir='logs') \n",
    "       \n",
    "    if opt.cuda:\n",
    "        print(\"=> Use GPU ID: '{}'\".format(opt.gpus))\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = opt.gpus\n",
    "        if not torch.cuda.is_available():\n",
    "            raise Exception(\"No GPU found or Wrong gpu id, please run without --cuda\")\n",
    "\t\t\n",
    "    torch.manual_seed(opt.seed)\n",
    "    if opt.cuda:\n",
    "        torch.cuda.manual_seed(opt.seed)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "   # Loading datasets\n",
    "    # train_set = TrainsetFromFolder('/media/hdisk/liqiang/hyperSR/train/'+ opt.datasetName + '/' +  str(opt.upscale_factor) + '/')\n",
    "    train_set = TrainsetFromFolder('/media/dy113/disk1/Project_xjr/dataset/hsisr_data/cave/cave_x4/train')\n",
    "    train_loader = DataLoader(dataset=train_set, num_workers=opt.threads, batch_size=opt.batchSize, shuffle=True)    \n",
    "    # val_set = ValsetFromFolder('/media/hdisk/liqiang/hyperSR/test/' + opt.datasetName + '/' + str(opt.upscale_factor))\n",
    "    val_set = TrainsetFromFolder('/media/dy113/disk1/Project_xjr/dataset/hsisr_data/cave/cave_x4/val')\n",
    "    val_loader = DataLoader(dataset=val_set, num_workers=opt.threads, batch_size=opt.batchSize, shuffle=False)\n",
    "      \n",
    "    # Buliding model     \n",
    "    model = LGFFMN()\n",
    "    criterion1 = nn.L1Loss() \n",
    "    criterion2 = SAMLoss() \n",
    "    \n",
    "    if opt.cuda:\n",
    "        model = nn.DataParallel(model).cuda()\n",
    "        criterion1 = criterion1.cuda()\n",
    "        criterion2 = criterion2.cuda()\n",
    "    else:\n",
    "        model = model.cpu()   \n",
    "    print('# parameters:', sum(param.numel() for param in model.parameters())) \n",
    "                   \n",
    "    # Setting Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(),  lr=opt.lr,  betas=(0.9, 0.999), eps=1e-08)    \n",
    "\n",
    "    # optionally resuming from a checkpoint\n",
    "    if resume:\n",
    "        if os.path.isfile(opt.resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(opt.resume))\n",
    "            checkpoint = torch.load(opt.resume)         \n",
    "            opt.start_epoch = checkpoint['epoch'] + 1 \n",
    "            model.load_state_dict(checkpoint['model'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(opt.resume))       \n",
    "\n",
    "    # Setting learning rate\n",
    "    scheduler = MultiStepLR(optimizer, milestones=[35, 70, 105, 140, 175], gamma=0.5, last_epoch=-1) \n",
    "    # scheduler = MultiStepLR(optimizer, milestones=[50, 100, 150, 200], gamma=0.5, last_epoch = -1) \n",
    "\n",
    "\n",
    "    # Val_psnr = []\n",
    "    # Training \n",
    "    for epoch in range(opt.start_epoch, opt.nEpochs + 1):\n",
    "        # scheduler.step()\n",
    "        print(\"Epoch = {}, lr = {}\".format(epoch, optimizer.param_groups[0][\"lr\"])) \n",
    "        train(train_loader, optimizer, model, criterion1, criterion2, epoch) \n",
    "        scheduler.step() \n",
    "        # Val_psnr = val(val_loader, model, epoch)    \n",
    "        val(val_loader, model, epoch)    \n",
    "\n",
    "        if (epoch) % 5 == 0:        \n",
    "            save_checkpoint(opt, epoch, model, optimizer)\n",
    "        # EarlyStopping(Val_psnr)\n",
    "        # if EarlyStopping.early_stop:\n",
    "        #     save_checkpoint(opt, epoch, model, optimizer)\n",
    "        #     print(\"Early stopping\")\n",
    "        #     break  \n",
    "\n",
    "def train(train_loader, optimizer, model, criterion1, criterion2, epoch):\n",
    "\n",
    "    model.train()   \n",
    "      \n",
    "    for iteration, batch in enumerate(train_loader, 1):\n",
    "        # input, label = Variable(batch[0]),  Variable(batch[1], requires_grad=False)\n",
    "        input, bicu, label = Variable(batch[0]),  Variable(batch[1]), Variable(batch[2], requires_grad=False)\n",
    "\n",
    "\n",
    "        if opt.cuda:\n",
    "            input = input.cuda()\n",
    "            bicu = bicu.cuda()\n",
    "            label = label.cuda()  \n",
    "\n",
    "            # SR, localFeats = model(x, y, i)    \n",
    "            SR = model(input, bicu)    \n",
    "\n",
    "            # localFeats.detach_()\n",
    "            # localFeats = localFeats.detach()\n",
    "            # localFeats = Variable(localFeats.data, requires_grad=False)\n",
    "            \n",
    "            loss = criterion1(SR, label) + criterion2(SR, label)\n",
    "            loss1 = criterion1(SR, label)\n",
    "            loss2 = criterion2(SR, label)\n",
    "\n",
    "            # loss = criterion1(SR, label) \n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()       \n",
    "            optimizer.step()         \n",
    "                                   \n",
    "        if iteration % 50 == 0:\n",
    "            # print(\"===> Epoch[{}]({}/{}): Loss: {:.10f}\".format(epoch, iteration, len(train_loader), loss.data[0]))\n",
    "            print(\"===> Epoch[{}]({}/{}): Loss: {:.10f} Loss1: {:.10f} Loss2: {:.10f}\".format(epoch, iteration, len(train_loader), loss.item(), loss1.item(), loss2.item()))\n",
    "            # print(\"===> Epoch[{}]({}/{}): Loss: {:.10f} \".format(epoch, iteration, len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "        # if opt.show:\n",
    "        #     writer.add_scalar('Train/Loss', loss.data[0], niter) \n",
    "        niter = epoch * len(train_loader) + iteration\n",
    "        if niter % 500 == 0:\n",
    "            # writer.add_scalar('Train/Loss', loss.data[0], niter)\n",
    "            writer.add_scalar('Train/Loss', loss.item(), niter) \n",
    "        # writer.add_scalar('Trainepoch/Loss', loss.item(), epoch)  \n",
    "\n",
    "def val(val_loader, model, epoch):\t            \n",
    "\n",
    "    model.eval()\n",
    "    val_psnr = 0    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for iteration, batch in enumerate(val_loader, 1):\n",
    "            # input, label = Variable(batch[0], volatile=True),  Variable(batch[1])\n",
    "            input, bicu, label = Variable(batch[0]),  Variable(batch[1]), Variable(batch[2], requires_grad=False)\n",
    "            SR = np.ones((label.shape[1], label.shape[2], label.shape[3])).astype(np.float32) \n",
    "        \n",
    "            if opt.cuda:\n",
    "                input = input.cuda()\n",
    "                bicu = bicu.cuda()\n",
    "    \n",
    "                # output, localFeats = model(x, y, localFeats, i)    \n",
    "\n",
    "                output = model(input, bicu)              \n",
    "          \n",
    "                SR = output.cpu().data[0].numpy()  \n",
    "  \t\n",
    "            val_psnr += PSNR(SR, label.data[0].numpy()) \n",
    "        val_psnr = val_psnr / len(val_loader) \n",
    "        print(\"PSNR = {:.3f}\".format(val_psnr))       \n",
    "        if opt.show:\n",
    "            writer.add_scalar('Val/PSNR',val_psnr, epoch) \n",
    "    # return val_psnr     \n",
    "    \n",
    "        \n",
    "def save_checkpoint(args, epoch, model, optimizer):\n",
    "    # model_out_path = \"checkpoint/\" + \"{}_model_{}_epoch_{}.pth\".format(opt.datasetName, opt.upscale_factor, epoch)\n",
    "    checkpoint_model_dir = './checkpoints/' + args.model_title + '/' + args.datasetName + '/' + str(args.upscale_factor) + '/' + 'nfeats128nblock4sub8_34_batch3200/'\n",
    "    if not os.path.exists(checkpoint_model_dir):\n",
    "        os.makedirs(checkpoint_model_dir)\n",
    "\n",
    "    ckpt_model_filename = args.datasetName + \"_\" + args.model_title + \"_ckpt_epoch_\" + str(epoch) + \".pth\"\n",
    "    ckpt_model_path = os.path.join(checkpoint_model_dir, ckpt_model_filename)\n",
    "    state = {\"epoch\": epoch , \"model\": model.state_dict(), \"optimizer\":optimizer.state_dict()}\n",
    "    # if not os.path.exists(\"checkpoint/\"):\n",
    "    #     os.makedirs(\"checkpoint/\")     \t\n",
    "    # torch.save(state, model_out_path)\n",
    "    torch.save(state, ckpt_model_path)\n",
    "    print(\"Checkpoint saved to {}\".format(ckpt_model_path))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> use gpu id: '2,3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dy113/.local/lib/python3.6/site-packages/ipykernel_launcher.py:50: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/dy113/.local/lib/python3.6/site-packages/ipykernel_launcher.py:53: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1512317657470703\n",
      "===The 1-th picture=====PSNR:35.088=====SSIM:0.9476=====SAM:4.153====Name:thread_spools_ms.mat\n",
      "0.14707088470458984\n",
      "===The 2-th picture=====PSNR:43.116=====SSIM:0.9906=====SAM:2.137====Name:real_and_fake_peppers_ms.mat\n",
      "0.14441347122192383\n",
      "===The 3-th picture=====PSNR:45.755=====SSIM:0.9941=====SAM:2.461====Name:real_and_fake_apples_ms.mat\n",
      "0.14646410942077637\n",
      "===The 4-th picture=====PSNR:41.272=====SSIM:0.9820=====SAM:2.524====Name:stuffed_toys_ms.mat\n",
      "0.1463935375213623\n",
      "===The 5-th picture=====PSNR:39.838=====SSIM:0.9845=====SAM:1.421====Name:sponges_ms.mat\n",
      "0.14647412300109863\n",
      "===The 6-th picture=====PSNR:42.689=====SSIM:0.9797=====SAM:5.835====Name:superballs_ms.mat\n",
      "=====averPSNR:41.293=====averSSIM:0.9798=====averSAM:3.089\n",
      "0.14700798193613687\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    # input_path = '/media/hdisk/liqiang/hyperSR/test/' + opt.datasetName + '/' + str(opt.upscale_factor) + '/' \n",
    "    input_path = '/media/dy113/disk1/Project_xjr/dataset/hsisr_data/cave/cave_x4/test/'\n",
    "    # out_path = '/media/hdisk/liqiang/hyperSR/result/' +  opt.datasetName + '/' + str(opt.upscale_factor) + '/' + opt.method + '/'\n",
    "    out_path = '/media/dy113/disk1/Project_xjr/hyperfusion/Mine/result/' + opt.model_title + '/' + opt.datasetName + '/' + str(opt.upscale_factor)  + '/' + 'nfeats128nblock4sub8_34_batch3200/'\n",
    "\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)      \n",
    "    PSNRs = []\n",
    "    SSIMs = []\n",
    "    SAMs = []\n",
    "\n",
    "\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "                    \n",
    "    if opt.cuda:\n",
    "        print(\"=> use gpu id: '{}'\".format(opt.gpus))\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = opt.gpus\n",
    "        if not torch.cuda.is_available():\n",
    "            raise Exception(\"No GPU found or Wrong gpu id, please run without --cuda\")\n",
    "\n",
    "    model = LGFFMN()\n",
    "\n",
    "    if opt.cuda:\n",
    "        model = nn.DataParallel(model).cuda()    \n",
    "        \n",
    "    checkpoint  = torch.load(opt.model_name)\n",
    "    model.load_state_dict(checkpoint['model']) \n",
    "    model.eval()       \n",
    "    \n",
    "    images_name = [x for x in listdir(input_path) if is_image_file(x)]  \n",
    "\n",
    "    with torch.no_grad():        \n",
    "        T = 0        \n",
    "        for index in range(len(images_name)):\n",
    "\n",
    "            mat = scio.loadmat(input_path + images_name[index]) \n",
    "            hyperLR = mat['LR'].astype(np.float32).transpose(2,0,1)\n",
    "            HR = mat['HR'].astype(np.float32).transpose(2,0,1)  \n",
    "\n",
    "            bic = np.zeros(HR.shape, dtype=np.float32)  \n",
    "\n",
    "            for i in range(bic.shape[0]):\n",
    "                # bic[i,:,:] = imresize(LR[i,:,:], (bic.shape[1], bic.shape[2]), 'bicubic', mode='F')  \n",
    "                # bic[i,:,:] = imresize(LR[i,:,:], (bic.shape[1], bic.shape[2]), 'bicubic', mode='F')  \n",
    "                bic[i,:,:] = np.array(Image.fromarray(obj=hyperLR[i,:,:], mode='F').resize(size=(bic.shape[1], bic.shape[2]), resample=Image.BICUBIC))        \n",
    "\n",
    "            bicu = Variable(torch.from_numpy(bic).float(), volatile=True).contiguous().view(1, -1, bic.shape[1], bic.shape[2])                  \t            \t      \t    \t        \t\n",
    "\n",
    "\n",
    "            input = Variable(torch.from_numpy(hyperLR).float(), volatile=True).contiguous().view(1, -1, hyperLR.shape[1], hyperLR.shape[2])      \n",
    "            HR = np.array(HR).astype(np.float32) \n",
    "         \n",
    "            if opt.cuda:\n",
    "                input = input.cuda() \n",
    "                bicu =bicu.cuda() \n",
    "           \n",
    "            # localFeats = []\n",
    "            start = time.time()\n",
    "     \n",
    "            output = model(input, bicu)                                 \n",
    "                             \n",
    "            SR = output.cpu().data[0].numpy().astype(np.float32)  \n",
    "                \n",
    "            end = time.time()   \n",
    "            print (end-start)\n",
    "            T = T + (end - start)\n",
    "                   \n",
    "            SR[SR<0] = 0             \n",
    "            SR[SR>1.] = 1.\n",
    "            psnr = PSNR(SR, HR)\n",
    "            ssim = SSIM(SR, HR)\n",
    "            sam = SAM(SR, HR)\n",
    "        \n",
    "            PSNRs.append(psnr)\n",
    "            SSIMs.append(ssim)\n",
    "            SAMs.append(sam)\n",
    "        \n",
    "            SR = SR.transpose(1,2,0)   \n",
    "            HR = HR.transpose(1,2,0)  \n",
    "        \n",
    "\t                    \n",
    "            scio.savemat(out_path + images_name[index], {'HR': HR, 'SR':SR})  \n",
    "            print(\"===The {}-th picture=====PSNR:{:.3f}=====SSIM:{:.4f}=====SAM:{:.3f}====Name:{}\".format(index+1,  psnr, ssim, sam, images_name[index]))                 \n",
    "        print(\"=====averPSNR:{:.3f}=====averSSIM:{:.4f}=====averSAM:{:.3f}\".format(np.mean(PSNRs), np.mean(SSIMs), np.mean(SAMs))) \n",
    "        print (T/len(images_name))\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XJR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "126e7dba9e31a2f4e2fdeaa18977348d147742daf45a57e7be21e8fe5c72a0e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
